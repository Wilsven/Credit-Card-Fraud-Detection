---
title: "Credit Card Fraud Detection"
author: "Wilsven Leong"
date: "July 11, 2021"
output: pdf_document
---

```{r Global Settings, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      fig.align = "center", 
                      fig.width = 6, 
                      fig.height = 3.5, 
                      options(digits = 2))
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
```

```{r, include = FALSE}
options(tinytex.verbose = TRUE)
```

```{r Load the Libraries}
# Require & Load Libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr",  repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales",  repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(geosphere)) install.packages("geosphere", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart",  repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest",  repos = "http://cran.us.r-project.org")

library(tidyverse)
library(data.table)
library(readr)
library(knitr)
library(kableExtra)
library(gridExtra) 
library(scales)
library(lubridate)
library(geosphere)
library(caret)
library(rpart)
library(randomForest)
library(tinytex)
library(rmarkdown)

options(scipen = 999)
options(timeout= 10000)
```

## Introduction

Credit card fraud losses total billions of dollars each year and is a major problem for financial institutions, customers and merchants. The 2018 Nilson Report estimated over $9 billion in fraudulent credit card transactions in the United State alone. Banks process thousands of transactions every minute and possess huge data sets, making good fraud detection models both necessary and possible. Since credit card companies cannot release real client data due to confidentiality, I chose a synthetic dataset from Kaggle to explore the issue. While synthetic data will not show real-world trends or unearth new predictors, it still provides excellent practice for analysis and machine learning modeling to detect anomalies and trends.

The dataset was generated using a Sparkov Data Generation Tool containing 23 real-life variables. It contains two years of data for 1000 cardholders. The set can be found at [kaggle.com/kartik2112/fraud-detection](https://www.kaggle.com/kartik2112/fraud-detection). The data has already been split into training and test sets. The training set is large with over 1.2 million rows.

I will explore the data to look for trends to detect fraud. Once relevant features are chosen, I will compare several algorithms presented in [HarvardX's Machine Learning course](https://rafalab.github.io/dsbook/examples-of-algorithms.html). The biggest problem for fraud detection models are imbalanced datasets. Credit card datasets are very large with few fraudulent transactions. Therefore, instead of overall accuracy, the focus of this project will be the process of choosing predictors, comparing and tuning algorithms for predicting the minority class and cost-saving results.

```{r Importing Data}
# datasets uploaded to github as specified in project instructions. 
# create temp file and download zip file containing dataset
dl <- tempfile()
download.file("https://github.com/Wilsven/Credit-Card-Fraud-Detection/releases/download/v1-files/fraud_data.zip", dl)

# unzip
unzip(dl)

# read & import csv files
fraudTest <- read_csv(unzip(dl,"fraudTest.csv"))
fraudTrain <- read_csv(unzip(dl,"fraudTrain.csv"))

# remove tempfile
rm(dl)
```

## Data Analysis

### Data Exploration

By examining the variables, we can see the variables are a mixture of date, categorical, numeric, and geospatial data: 

```{r Train Dataset, comment=""}  
# exploring data set variables & structure
glimpse(fraudTrain)
```  

The variables are a mixture of customer, merchant and transaction specific data. Customer related variables include: first & last name, gender, multiple columns of address information, date of birth, and job. Transaction variables are: date and time, card number, purchase category, amount, id, unix time, and if fraud. Merchant variables include: name, latitude and longitude. There is also a row id.  
  
There are quite a few redundant variables. First/last names and cc_num all identify for the individual account. There are seven variables related to the account address: street address, city, zip code, state, city population, longitude and latitude. The street address of a customer can be a good predictor of fraud. It isn't a logical predictor in this dataset since the is no transaction processing data included. Instead, I will focus on cc_nums, longitude, and latitude.  
  
Summary of numeric and date variables:  
  
```{r Train Datset Summary, comment=""}  
# exploring date & numeric variables
fraudTrain[c(2:3,6,13,16,18,23)] %>% 
  summary()
```

The summary provides several important revelations. Even though transaction amounts have a large range from \$1 to ~\$28,000, the mean is only \$70. Since the third quartile starts at \$83, it is apparent that most transactions are under \$100.   

The date range is 2019-01-01 to 2020-06-21 not the expected two years. The Kaggle page stated the data covered two years from 2019 to 2020. However, when you check the date ranges, you can see that the training and test sets were split by dates. This meant that the test set consists of transactions in the last six months.  

Training set: `r range(fraudTrain$trans_date_trans_time)`. 

Test set: `r range(fraudTest$trans_date_trans_time)`.  
  
This seems like a poor sampling method especially for time trends. Therefore, I will recombine the data from the train and test sets and re-partition into 80/20 training and test splits based on random sampling method. Let's check for NA values in is_fraud column.

```{r Recreating Training & Test Sets}
# Data was pre-split by date. Merging together to randomly split. Must first remove X1 row number to prevent duplicates.
fraudTest <- fraudTest[-1]
fraudTrain <- fraudTrain[-1]
fraudSet <- rbind(fraudTrain, fraudTest)

# removing pre-split sets
rm(fraudTest, fraudTrain)

# check for NA values in is_fraud column, if present, drop the row(s)
sum(is.na(fraudSet$is_fraud))
index <- which(is.na(fraudSet$is_fraud))
fraudSet <- fraudSet[-index,]

## Splitting data set options
# Creating Random Sampling of Training & 20% Final Test set of fraud data. 
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = fraudSet$is_fraud, times = 1, p = 0.2, list = FALSE)
train_set <- fraudSet[-test_index,]
test_set <- fraudSet[test_index,]
```

Once resampled, there are now both training and test sets covering two years. Tabulating the proportion of is_fraud shows 0.52% transactions of transactions are fraudulent.

```{r Proportion of Fraudulent Transactions in Train & Test sets}
# proportion of fraudulent transactions in training and test sets
prop.table(table(train_set$is_fraud)) #.52%

# removing
rm(fraudSet)
rm(index)
```

Now that the training set is re-partitioned, let's explore the difference between legitimate and fraudulent transactions to better understand the data.

```{r Legitimate vs Fraud Transactions}
kable(train_set %>%
  group_by(is_fraud) %>%
  summarize(avg_trans = mean(amt),
            med_trans = median(amt),
            amt = sum(amt),
            n = n()) %>%
  mutate(pct_amt = (amt/sum(amt))*100,
         pct_n = (n/sum(n))*100), digits = 2) %>%
  kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

The cost of fraud in the training set is ~\$4,000,000 and makes up 4.0% of the total amount transacted. This is eight times higher than the percent of number of transactions. The average fraud transaction is also much higher at ~$500, making amount a likely predictor. Since the average amount for fraudulent transactions is much higher, it is important to investigate further and summarize the transaction amounts into segments (bins).  

```{r Bins Summary}
# Summary of Bins by Fraud
kable(train_set %>%
  mutate(bins = cut(amt, breaks = c(-Inf, 100, 1000, Inf), 
                      labels = c("<$100", "$100-$999", ">$1K+"))) %>%
  group_by(bins, is_fraud) %>%
  summarize(amt = sum(amt),
            n = n()) %>%
  mutate(pct_amt = (amt/sum(amt))*100,
         pct_n = (n/sum(n))*100), digits = 2) %>%
  kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

Since over 1.2 million transactions are under $100 this segment will overwhelm the visualization of the amount distribution if viewed together. When transaction amounts are segmented into bins and the scales are allowed to float to its respective bins, a better understanding of the distributions can be achieved.    

```{r Histograms of Distributions of Transaction Amounts, fig.width = 6.5, fig.height = 2.25}
# Plot histogram distributions of legitimate transaction amounts
train_set %>%
  filter(is_fraud == 0) %>%
  mutate(bins = cut(amt, breaks = c(-Inf, 100, 1000, Inf), 
                    labels = c("<$100", "$100-$999", ">$1K+"))) %>%
  ggplot(aes(amt)) +
  geom_histogram(bins = 40, fill = "#56B4E9") +
  facet_wrap(~ bins, scales = "free") +
  labs(title = "Legitimate Transaction Amounts",
       x = "Amount ($)", y = "No. of Transactions") +
  theme_bw(base_size = 10) +
  theme(plot.title = element_text(size = 10))

# Plot histogram distributions of fraudulent transaction amounts
train_set %>%
  filter(is_fraud == 1) %>%
  mutate(bins = cut(amt, breaks = c(-Inf, 100, 1000, Inf), 
                    labels = c("<$100", "$100-$999", ">$1K+"))) %>%
  ggplot(aes(amt)) +
  geom_histogram(bins = 40, fill = "#56B4E9") +
  facet_wrap(~ bins, scales = "free") +
  labs(title = "Fraudulent Transaction Amounts",
       x = "Amount ($)", y = "No. of Transactions") +
  theme_bw(base_size = 10) +
  theme(plot.title = element_text(size = 10))
```

While the majority of legitimate transactions are under \$100, 78.2% of fraudulent transactions are over \$100. The amount of the transaction is definitely a predictor.  

Next, let's explore categories. What are the actual transaction amounts by category, and how do they vary?  

```{r Transaction Amounts by Category, fig.width = 7, fig.height = 4.5}
# Legitimate Transaction Amounts by Category
x <- train_set %>%
  filter(is_fraud == 0) %>%
  ggplot(aes(amt)) +
  geom_histogram(bins = 40, fill = "#56B4E9") +
  facet_wrap(~ category) +
  labs(title = "Legitimate Transaction Amounts by Category",
       x = "Amount ($)", y = "No. of Transactions") +
  theme_bw(base_size = 10) +
  theme(plot.title = element_text(size = 10), 
        axis.text.x = element_text(angle = 90))

# Fraudulent Transaction Amounts by Category
y <- train_set %>%
  filter(is_fraud == 1) %>%
  ggplot(aes(amt)) +
  geom_histogram(bins = 40, fill = "#56B4E9") +
  facet_wrap(~ category) +
  labs(title = "Fraudulent Transaction Amounts by Category",
       x = "Amount ($)", y = "No. of Transactions") +
  theme_bw(base_size = 10) +
  theme(plot.title = element_text(size = 10))

# arrange histograms side by side
grid.arrange(x, y, ncol = 2)
# remove variable
rm(x, y)
```

The distributions for transaction amount by category differ drastically for fraudulent transactions but remains similar otherwise.  

```{r Barchart of Fraud Range by Category, fig.width = 7, fig.height = 5}
# Barchart with Fraud & Legit Transactions by Category 
train_set %>%
  group_by(category, is_fraud) %>%
  summarize(amt = sum(amt), n = n()) %>%
  ggplot(aes(x = amt, y = reorder(category, amt), fill = as.factor(is_fraud))) +
  geom_bar(stat = "identity") +
  labs(title = "Transaction Amounts by Category",
       x = "Amount ($)", y = "Category",
       fill = "") +
  scale_fill_manual(values = c("grey68","darkorange2"),
                    labels = c("Legitimate", "Fraudulent")) +
  scale_x_continuous(labels = comma) +  
  theme_bw(base_size = 10) +
  theme(plot.title = element_text(size = 10),
        legend.key.height = unit(.4, "cm"),
        legend.position = "bottom")
```

The plot above shows that fraudulent transactions are very specific to certain ranges within categories. Looking at the fraudulent amounts range by category, it is possible to use amount bins to better predict fraud.  

```{r Boxplot of Fraud Range by Category, fig.width = 7, fig.height = 5}
# Boxplot with Fraud & Legit Transactions by Category 
train_set %>%
  filter(amt < 1200) %>%
  mutate(is_fraud = as.factor(is_fraud)) %>%
  group_by(is_fraud, category) %>%
  ggplot(aes(x = is_fraud, y = amt, colour = is_fraud)) +
  geom_boxplot(lwd = .5,
               outlier.shape = .75,
               outlier.alpha = .3) +
  facet_wrap(~ category, ncol = 4) +
  labs(title = "Transaction Amounts by Category: Legitimate vs Fraudulent", 
       y = "Amount ($)",
       colour = "") +
  scale_y_continuous(breaks = c(0,250,500,750,1000)) +
  scale_colour_manual(values = c("grey68","darkorange2"),
                    labels = c("Legitimate", "Fraudulent")) +
  theme_bw(base_size = 10) +
  theme(legend.position = "bottom", 
        panel.grid.minor = element_blank(), 
        axis.text.x=element_blank(), 
        axis.title.x=element_blank(),  
        plot.title = element_text(size = 10))
```

```{r Fraud Amount by Category, fig.width = 7, fig.height = 4}
# (Grid) Legitimate & Fraudulent Breakdown per Category by % Amount 
train_set %>%
  mutate(bins = cut(amt, breaks = c(-Inf, 100, 250, 800, 1400, Inf),
         labels = c("<$100", "$100-$249", "$250-$799", "$800-$1400", "$1.4K+")),
         is_fraud = as.factor(is_fraud)) %>%
  group_by(is_fraud, category, bins) %>%
  summarize(amt = sum(amt)) %>%
  mutate(pct_amt = (amt/sum(amt))) %>%
  ggplot(aes(x = pct_amt, y = reorder(category, pct_amt), fill = is_fraud)) +
  geom_col(position = position_dodge2(width = 0.9, preserve = "single")) +
  facet_wrap(~ bins, ncol = 5) +
  scale_fill_manual(values = c("grey68","darkorange2"),
                      labels = c("Legitimate", "Fraudulent")) +
  labs(title = "Breakdown of Transaction Amounts by Category",
       x = "% of Amount", y = "Category", fill = "") +
  theme_bw(base_size = 10) +
  theme(legend.position = "bottom", 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 90),
        legend.key.height = unit(.4, "cm"),
        plot.title = element_text(size = 10))
```

The transaction amounts and categories are very good predictors in this dataset.  
  
It is very likely that date and time elements also provide important insights. Therefore, visualizing all transactions on an account with fraudulent transactions will help get an idea of the relationship between timing and the nature of transactions.

```{r Timeline of an Account with Fraudulent Transactions, fig.width = 5, fig.height = 2.75}
# Pick the tenth (randomly selected) credit card with fraudulent transactions
cc_number <- train_set[which(train_set$is_fraud == 1),][10,]$cc_num

# Plot a scatterplot as a timeline to visualize nature of transactions
train_set %>%
  filter(cc_num == cc_number) %>%
  mutate(trans_date = as_date(trans_date_trans_time),
         is_fraud = as.factor(is_fraud)) %>%
  ggplot(aes(x = trans_date, y = amt, colour = is_fraud)) +
  geom_point() + 
  scale_colour_manual(values = c("grey68","darkorange2"),
                    labels = c("Legitimate", "Fraudulent")) +
  labs(title = "Example Timeline of CC with Fraudulent Transactions",
       x = "Date", y = "Amount ($)", colour = "") +
  theme_bw(base_size = 10) +
  theme(legend.position = "bottom", 
        panel.grid.minor = element_blank(),
        plot.title = element_text(size = 10))
```

From the scatter plot above, it appears as though fraudulent transactions happen in batches in very quick successions (i.e. within a couple of days). This shows that timing and frequency of transactions are potential predictors. 

Therefore, the date and time column should be explored more. We can first explore transaction timings. When visualizing the percentages of transaction amounts that are fraudulent over a timescale of an hour interval throughout the day, it seems that between 10PM to 3AM, fraudulent transaction amounts are substantially higher. 

The likelihood for fraudulent transactions is especially prevalent between 10PM to 11PM, as shown in the plot below. With approximately 30.0% of transaction amounts being frauds, this is as much as 30 times more than other timings between 4AM to 9PM.   

```{r Total Fraudulent Transactions Amounts throughout the day}
# Plot Percentage of Fraudulent Transactions Amounts throughout the Day
train_set %>%
  mutate(trans_hour = hour(trans_date_trans_time)) %>%
  group_by(trans_hour, is_fraud) %>%
  summarize(amt = sum(amt)) %>%
  mutate(pct_amt = (amt/sum(amt))*100) %>%
  filter(is_fraud == 1) %>%
  ggplot(aes(x = trans_hour)) +
  geom_bar(aes(y = pct_amt, fill = pct_amt), stat = "identity") +
  geom_text(aes(y = pct_amt, label = sprintf("%1.1f%%", pct_amt)), size = 2, nudge_y = 1) +
  scale_x_continuous(breaks = seq(0, 23),
                     labels = c("12AM", "1AM", "2AM", "3AM", "4AM", "5AM", "6AM", "7AM", "8AM",
                                "9AM", "10AM", "11AM", "12PM", "1PM", "2PM", "3PM", "4PM", "5PM", 
                                "6PM", "7PM", "8PM", "9PM", "10PM", "11PM")) +
  scale_y_continuous(labels = function(x){
    paste0(x,"%")}, 
    limits = c(0,40)) + 
  scale_fill_continuous(labels = function(x)
    {paste0(x, "%")},
    limits = c(0,40)) +
  labs(title = "% of Fraudulent Transaction Amounts throughout the Day",
       x = "Time", y = "% of Total Fraudulent Transactions", fill = "Percentage") +
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        plot.title = element_text(size = 10),
        axis.text.x = element_text(angle = 90))
```

Unsurprisingly, if we plot the number of transactions that are fraudulent, we can see that the proportion of fraudulent transactions between 10PM to 3AM occur at higher rates (approximately 10 times) than other times throughout the day.

```{r Number of type of Transactions throughout the day}
# Plot Number of Fraudulent Transactions throughout the Day
train_set %>%
  mutate(trans_hour = hour(trans_date_trans_time)) %>%
  group_by(trans_hour, is_fraud) %>%
  summarize(n = n()) %>%
  mutate(pct_n = (n/sum(n))*100) %>%
  ggplot(aes(x = trans_hour, y = pct_n, colour = as.factor(is_fraud))) +
  geom_point() +
  geom_text(aes(y = pct_n, label = sprintf("%1.1f%%", pct_n)), size = 3.5, nudge_y = 1, vjust = -1) +
  scale_x_continuous(breaks = seq(0, 23),
                     labels = c("12AM", "1AM", "2AM", "3AM", "4AM", "5AM", "6AM", "7AM", "8AM",
                                "9AM", "10AM", "11AM", "12PM", "1PM", "2PM", "3PM", "4PM", "5PM", 
                                "6PM", "7PM", "8PM", "9PM", "10PM", "11PM")) +
  scale_y_continuous(labels = function(x){
    paste0(x,"%")}) + 
  scale_colour_manual(values = c("grey68","darkorange2"),
                      labels = c("Legitimate", "Fraudulent")) +
  labs(title = "% of Types of Transactions throughout the Day",
       x = "Time", y = "% of Transactions", colour = "") +
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank(),
        plot.title = element_text(size = 10),
        axis.text.x = element_text(angle = 90))
```

Next, let us take a look into the monthly proportion of fraudulent transaction amounts. 

```{r Proportion of Fraudulent Transaction Amounts during each Month}
# Load lubridate library 
library(lubridate)

# Plot Proportion of Fraudulent Transaction Amounts during each Month
train_set %>%
  mutate(trans_month = month(trans_date_trans_time, label = TRUE)) %>%
  group_by(trans_month, is_fraud) %>%
  summarize(amt = sum(amt)) %>%
  mutate(pct_amt = (amt/sum(amt))*100) %>%
  filter(is_fraud == 1) %>%
  select(-is_fraud, -amt) %>%
  ggplot(aes(x = trans_month)) +
  geom_bar(aes(y = pct_amt, fill = pct_amt), stat = "identity") +
  geom_text(aes(y = pct_amt, label = sprintf("%1.2f%%", pct_amt)), size = 3.5, vjust = -1) +
  scale_y_continuous(labels = function(x){
    paste0(x,"%")}, 
    limits = c(0,8)) + 
  scale_fill_continuous(labels = function(x)
  {paste0(x, "%")},
  limits = c(0,8)) +
  labs(title = "% of Fraudulent Transaction Amount for each Month",
       x = "Month", y = "% of Fraudulent Transaction Amounts", fill = "Percentage") +
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        plot.title = element_text(size = 10))
```

While the monthly trend in the plot above is less obvious than the hourly trend, we can still clearly see that there are higher percentages of fraudulent transaction amounts in the months of January, February and May.

```{r Number of Types of Transactions for each Month}
# Plot Number of Types of Transactions for each Month
train_set %>%
  mutate(trans_month = month(trans_date_trans_time, label = TRUE)) %>%
  group_by(trans_month, is_fraud) %>%
  summarize(n = n()) %>%
  mutate(pct_n = (n/sum(n))*100) %>%
  ggplot(aes(x = trans_month, y = pct_n, colour = as.factor(is_fraud))) +
  geom_point() +
  geom_text(aes(y = pct_n, label = sprintf("%1.2f%%", pct_n)), size = 3.5, nudge_y = 1, vjust = -1) +
  scale_y_continuous(labels = function(x){
    paste0(x,"%")}) + 
  scale_colour_manual(values = c("grey68","darkorange2"),
                      labels = c("Legitimate", "Fraudulent")) +
  labs(title = "% of Types of Transactions for each Month",
       x = "Month", y = "% of Transactions", colour = "") +
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank(),
        plot.title = element_text(size = 10))
```

The trend for the monthly proportion for the types of transactions is even harder to pinpoint than the previous plot. However, there are some obvious months where the proportion for fraudulent transactions are higher, like in January and February. 

Next, let us take a look at how certain days of a month relate to fraudulent transactions.

```{r Proportion of Fraudulent Transaction Amounts during each Day of the Month}
# Plot Proportion of Fraudulent Transaction Amounts during each Day of the Month
train_set %>%
  mutate(trans_day = day(trans_date_trans_time)) %>%
  group_by(trans_day, is_fraud) %>%
  summarize(amt = sum(amt)) %>%
  mutate(pct_amt = (amt/sum(amt))*100) %>%
  filter(is_fraud == 1) %>%
  select(-is_fraud, -amt) %>%
  ggplot(aes(x = trans_day)) +
  geom_bar(aes(y = pct_amt, fill = pct_amt), stat = "identity") +
  geom_text(aes(y = pct_amt, label = sprintf("%1.2f%%", pct_amt)), size = 2.5, vjust = -1) +
  scale_x_continuous(breaks = seq(1,31,1)) +
  scale_y_continuous(labels = function(x){
    paste0(x,"%")}, 
    limits = c(0,6)) + 
  scale_fill_continuous(labels = function(x)
  {paste0(x, "%")},
  limits = c(0,6)) +
  labs(title = "% of Fraudulent Transaction Amounts for each Day of the Month",
       x = "Day", y = "% of Fraudulent Transaction Amounts", fill = "Percentage") +
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        plot.title = element_text(size = 10))
```

```{r Number of Fraudulent Transactions for each Day of the Month}
# Plot Number of Fraudulent Transactions for each Day of the Month
train_set %>%
  mutate(trans_day = day(trans_date_trans_time)) %>%
  group_by(trans_day, is_fraud) %>%
  summarize(n = n()) %>%
  mutate(pct_n = (n/sum(n))*100) %>%
  ggplot(aes(x = trans_day, y = pct_n, colour = as.factor(is_fraud))) +
  geom_point() +
  geom_text(aes(y = pct_n, label = sprintf("%1.1f%%", pct_n)), size = 3, nudge_y = 1, vjust = -1) +
  scale_x_continuous(breaks = seq(1,31,1)) +
  scale_y_continuous(labels = function(x){
    paste0(x,"%")}) + 
  scale_colour_manual(values = c("grey68","darkorange2"),
                      labels = c("Legitimate", "Fraudulent")) +
  labs(title = "% of Types of Transactions for each Day of the Month",
       x = "Day", y = "% of Transactions", colour = "") +
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank(),
        plot.title = element_text(size = 10))
```

The trend for the proportion of types of transactions for each day of the month is difficult to spot. In the plot for proportion of fraudulent transaction amounts for each day of the month, we can see that some days have higher proportions of fraud. 

While there may be a slight trend, it is not obvious and just like the monthly visualizations, it seems that the best date time predictor for fraudulent transactions is the hourly timescale.

Similarly, there are variables with zero predictive power as well. For instance, neither gender nor age appear to be predictors for fraudulent transactions.

```{r Fraudulent Transactions by Gender}
kable(train_set %>%
  group_by(is_fraud, gender) %>%
  summarize(amt = sum(amt), n = n()) %>%
  mutate(pct_amt = (amt/sum(amt))*100,
         pct_n = (n/sum(n))*100) %>%
  filter(is_fraud == 1), digits = 2) %>%
  kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

```{r Fraud by Y.O.B, fig.width = 5, fig.height = 1.75}
# Plot histogram of Fraud by Y.O.B
train_set %>%
  ggplot(aes(dob)) +
  geom_histogram(bins = 40, colour = "white", fill = "#56B4E9") +
  facet_wrap(~ is_fraud, scales = "free") +
  labs(title = "Histogram of Transactions by Y.O.B",
       x = "Year", y = "Count") +
  theme_bw(base_size = 10) +
  theme(plot.title = element_text(size = 10))
```

In most cases, fraudulent transactions are often between merchants and customers who are very far from one another. Using the longitude and latitude data for both the customer and the merchant, we can calculate and analyze distance.  

```{r Trans Distance by Long Lat}
# Calculate distance between cust & merchant with longitude and latitude 
train_distance <- train_set[c(1:5,12:14,20:22)] %>% 
  rowwise() %>% 
  mutate(trans_dist = distHaversine(c(long, lat), 
                                    c(merch_long, merch_lat))/ 1609)
```

In this synthetic dataset, all transactions are within 100 miles of the customer address, and no trend for fraud can be can be ascertained.  Transaction distances Range: Min: `r min(train_distance$trans_dist)`  Max: `r max(train_distance$trans_dist)` 

```{r Distribution of distances, fig.width = 5, fig.height = 2}
# Distribution of distances
train_distance %>% 
  ggplot(aes(trans_dist)) + 
  geom_histogram(bins = 40, colour = "white", fill = "#56B4E9") + 
  facet_wrap(~ is_fraud, scales = "free") +
  labs(title = "Transaction Distances",
       x = "Miles between Customer / Merchant", y = "") +
  theme_bw(base_size = 10) +
  theme(plot.title = element_text(size = 10))

rm(train_distance)
```

Lastly when looking at the breakdown of fraud by customer state, fraud rates are consistent with percent of accounts by state. 

```{r Percentages of Fraudulent Amounts & Breakdown of Accounts by State, fig.width = 7, fig.height = 5.75}
# Plot Percentage of Fraudulent Transaction Amounts by State
x <- train_set %>%
  group_by(is_fraud, state) %>%
  summarize(amt = sum(amt), n = n()) %>%
  mutate(pct_amt = (amt/sum(amt))*100,
         pct_n = (n/sum(n))*100) %>%
  filter(is_fraud == 1)  %>%
  ggplot(aes(x = pct_amt, y = reorder(state, pct_amt))) +
  geom_bar(width = 0.8, fill = "#56B4E9", stat = "identity") +
  geom_text(aes(label = sprintf("%1.1f%%", pct_amt)), 
            size = 2.5, nudge_x = 0.25, alpha = 0.5) +
  labs(title = "% of Fraudulent Transaction Amounts by State",
       x = "Percentage", y = "") +
  theme_bw(base_size = 10) +
  theme(axis.title.x=element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank(),
        plot.title = element_text(size = 10))
  
# Plot Breakdown of Accounts by State
y <- train_set %>%
  group_by(state) %>%
  summarize(account = n_distinct(cc_num), amt = sum(amt), n = n()) %>%
  mutate(pct_account = (account/sum(account))*100) %>%
  ggplot(aes(x = pct_account, y = reorder(state, pct_account))) +
  geom_bar(width = 0.8, fill = "#56B4E9", stat = "identity") +
  geom_text(aes(label = sprintf("%1.1f%%", pct_account)), 
            size = 2.5, nudge_x = 0.25, alpha = 0.5) +
  labs(title = "Breakdown of Accounts by State",
       x = "Percentage", y = "") +
  theme_bw(base_size = 10) +
  theme(axis.title.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank(),
        plot.title = element_text(size = 10))

grid.arrange(x, y, ncol = 2)
rm(x, y)
```

### Data Preparation

Before building the models, the dataset needs to be prepped to include the relevant predictors. To reduce the size of the dataset, unused columns will be removed and the date and time elements converted into features. Predictors like amount, category, transaction date and time (hour, month, day of month, and weekday) will be kept. The merchant and cc_num will be kept as possible predictors. 

Unfortunately, the synthetic card numbers were created fictionally which reduced their utility for modeling. In reality card numbers are between 13 to 16 digits. The first digit represents the card type and digits two through six identify the institution and the final digits are unique account ids. The credit card numbers in this dataset are between 11 to 19 digits and do not follow convention. Therefore, the cc_num will be converted to a character variable to be handled appropriately by the algorithms. It is important to note that by using unique identifiers such as account numbers, it can lead to over training. Since frauds are usually repeated, cc_num has usable predicting power.

To effectively train and tune models, it is necessary to partition the training set to avoid over-training. A 90/10 train/test split will be used to include as much of the data as possible since there are so few fraudulent transactions.
  
```{r Data Preparation}
# Prepping Final Training Set 

# Creating prepped training set 
# Removing columns not used in modeling
# Removing fraud from merchant
# Added transaction amount bins
train_set <- train_set[c(1:5,22)] 

# Removing columns not used in modeling
# Removing fraud from merchant
# Added transaction amount bins capped at 100000 as if credit limit
train_set <- train_set %>% 
  mutate(merchant = str_remove(merchant, "fraud_"), 
         cc_num = as.character(cc_num), 
         is_fraud = as_factor(is_fraud),
         bins = cut(amt, breaks = c(0,99,249,799,1399,100000),
                    labels = c("<$100","$100-$249","$250-$799","$800-$1400","$1400+")))

# Separating transaction date & time & converting day and trans_hour to factors
# This runs much faster when saved to different object
train_prep <- train_set %>% 
  mutate(month = month(trans_date_trans_time, label = TRUE), 
         day = day(trans_date_trans_time), 
         hour = hour(trans_date_trans_time), 
         weekday = wday(trans_date_trans_time, label = TRUE)) %>% 
  mutate(day = as_factor(day), hour = as_factor(hour))

# Splitting data again at 90/10: for modeling training & testing
# Leaving test_set for final evaluation of models
set.seed(1, sample.kind = "Rounding")
model_index <- createDataPartition(y = train_prep$is_fraud, times = 1, p = 0.1, list = FALSE)
train2 <- train_prep[-model_index,]
test2 <- train_prep[model_index,]

# Proportion of fraudulent transactions
table(train2$is_fraud) # 0.52%
table(test2$is_fraud) # 0.52%

# Removing initial full data set 
rm(train_set, train_prep)

# Prepping Final Validation Test Set 

# Creating prepped validation set 
# Removing columns not used in modeling
# Removing fraud from merchant
# Added transaction amount bins
test_set <- test_set[c(1:5,22)]

# Removing columns not used in modeling
# Removing fraud from merchant
# Added transaction amount bins capped at 100000 as if credit limit
test_prep <- test_set %>%
  mutate(merchant = str_remove(merchant, "fraud_"),
         cc_num = as.character(cc_num),
         is_fraud = as.factor(is_fraud),
         bins = cut(amt, breaks = c(0,99,249,799,1399,100000), 
                    labels = c("<$100","$100-$249","$250-$799","$800-$1400","$1400+")))

# Separating transaction date & time & converting day and trans_hour to factors
# This runs much faster when saved to different object
test_set <- test_prep %>%
  mutate(month = month(trans_date_trans_time, label = TRUE),
         day = day(trans_date_trans_time),
         hour = hour(trans_date_trans_time),
         weekday = wday(trans_date_trans_time, label = TRUE)) %>%
  mutate(day = as.factor(day), hour = as.factor(hour))

# Removing test_prep
rm(test_prep, model_index, test_index)
```

\  
\newpage  

## Modeling Methods 

Three algorithms best for anomaly predictions will be presented: two Classification models and a Regression Tree (CART) model. The models used for this are rpart, randomForest and logistic regression model using glm. CART algorithms work by predicting an outcome or classification and are commonly used in fraud detection. Logistic regression models use linear regression to determine the probability of a binary outcome.  

**Rpart**:  
Rpart creates a decision tree through Recursive PARTitioning to predict the class of the target variable. Rpart repeatedly subsets predictors into non-overlapping regions (partitions) at decision nodes which create the largest and most uniform subset. This can be described as partition $\textbf{x}$, predictor $\textit{j}$, and value $\textit{s}$ where rpart splits observations into two regions $R_{1}(\textit{j,s})$ and $R_{2}(\textit{j,s})$.  Mathematically represented as:

$$R_{1}(\textit{j,s}) = \left \{ \textbf{x} \mid \textit{x}_{j} < \textit{s} \right \} \:\:\: and \:\:\:  R_{2}(\textit{j,s}) = \left \{ \textbf{x} \mid  \textit{x}_{j} \geq \textit{s} \right \}$$ 

Rpart chooses $\textit{j}$ and $\textit{s}$ which minimize the residual sum of squares (RSS). Partitioning continues until minimum value of improvement in RSS, referred to as complexity parameter (cp), is reached. Rparts cp default is .01 but can be tuned.$^{[1] [2]}$

**Random Forest**:  
RandomForest is an ensemble CART algorithm which creates large numbers of decision trees with different subsets of variables then aggregates the predictions. The algorithm builds $\textit{B}$ trees resulting in models $\textit{T}_{1}, \textit{T}_{2}, ... , \textit{T}_{B}$. For each observation randomForest, predicts $\hat{y}_{j}$ from $\textit{T}_{j}$. In a classification outcome, prediction $\hat{y}$ is the majority vote among $\hat{y}_{1},...\hat{y}_{T}$. Both the number of trees (ntree) and number of variables to use per tree (mtry) are editable parameters and can be tuned. The defaults are 500 trees and square root of number of variables.$^{[3] [4]}$
  
**Logistic Regression**:  
The glm function will be used to compute a logistic regression model which predicts the conditional probability of an outcome: $Pr(Y=1|X=\textit{x})$. To ensure the estimate is between 0 to 1, glm's family function is set l to binomial to apply the logit transformation, $g(p) = log\frac{p}{1-p}$. This will create a regression model: 

$$g\left \{ p(\textit{x}_{1},\textit{x}_{2},..,\textit{x}_{n}) \right \} = g\left \{ Pr(Y=1\mid X_{1}=\textit{x}_{1},X_{2}=\textit{x}_{2},...,X_{n}=\textit{x}_{n}) \right \} = \beta_{0} + \beta_{1}\textit{x}_{1} + \beta_{2}\textit{x}_{2} + ... + \beta_{n}\textit{x}_{n}$$ 

After the model fits estimates for $\beta_{0} + \beta_{1}\textit{x}_{1} + ... + \beta_{n}\textit{x}_{n}$, the predict.glm function calculates the conditional probabilities. To obtain a prediction, I must define a decision rule to produce a vector of predicted outcomes based on the threshold (such as $\hat{y}$ > .5).  

Logistic regression is limited in its modeling capability.  Simply stated, glm calculates the relationship between features and outcome on a linear plane which means it cannot model non-linear relationships.$^{[5]}$  Furthermore, glm cannot handle categorical variables with many levels. Although a limited and simplistic approach, glm does allow the algorithm to model interaction between variables. Based on the above analysis, the relationship between category and amount appears to be the best indicator of fraud in the data. In this case the equation will change to include an coefficient for the interaction for amount and category:  

$$g\left \{ p(\textit{amt},cat) \right \} = \beta_{0} + \beta_{1}\textit{amt} + \beta_{2}\textit{cat} + \beta_{3}\textit{amt}\ast\textit{cat}$$. 
  
Modeling Issues: The caret function was used to train all models by utilizing its cross-validation feature. Unfortunately, with over a million observations, caret took hours to run and did not significantly improve the model as compared to rpart and glm. 

\newpage  

### Model Evaluation

Accuracy is a poor evaluation metric for imbalanced datasets. For instance, this dataset contains only 0.5% fraudulent transactions, even a model that predicts no fraud will have a 99.5% accuracy. Credit card companies utilize fraud detection algorithms to prevent revenue loss. As shown during analysis, the percentage of fraudulent transaction amounts (cost) is eight times higher than the number of fraudulent transactions. In real life, fraudulent charges also produce additional customer service costs.$^{[6]}$ Therefore, a combination of confusion matrix metrics and cost analysis will be used to evaluate model performance.  
  
Predictions have four possibilities in the following: 

  * True Positive (TP): legitimate predicted for actual legitimate transaction
  * False Positive (FP): legitimate predicted for actual fraudulent transaction
  * False Negative (FN): fraud predicted for actual legitimate transaction
  * True Negative  (TN): fraud predicted for actual fraudulent transaction  

In the confusion matrix of a fraud detection model, is_fraud = 0 is a legitimate transaction (positive outcome), and is_fraud = 1 is fraudulent (negative outcome).  

```{r Confusion Matrix, fig.width = 2, fig.height = 1.25}
# Create a dataframe of prediction possibilities
Actual <- factor(c(0,0,1,1))
Predicted <- factor(c(0, 1, 0, 1))
Y <- c("TP","FN","FP","TN")
cm_df <- data.frame(Actual, Predicted, Y)

# Create a tile plot
cm_df %>%
  ggplot(aes(x = Actual, y = reorder(Predicted, desc(Predicted)), labels = Y)) +
  geom_tile(aes(fill = Y)) +
  geom_text(label = Y, size = 3) +
  scale_fill_brewer() +
  labs(x = "Actual",y = "Predicted") +  
  theme_bw(base_size = 9) + 
  theme(legend.position = "none")
```

Evaluation Metrics: 

  * Specificity: The proportion correct fraud predictions to actual fraud, also called True Negative Rate (TNR). Specificity in an imbalanced dataset is a better metric than accuracy. The formula is as shown: $TN/(TN + FP)$.
  * Negative Predictive Value (NPV): The proportion correct fraud predictions to all fraud predicted. NPV shows if the model is incorrectly identifying legitimate transactions. The formula is as shown: $TN/(TN + FN)$.
 
Costs:

  * Amount Saved: Amount ($) of fraud correctly predicted (TN)
  * Fraud Missed: Amount ($) of fraud missed or loss (FP)
  * Misclassified: Amount ($) incorrectly predicted as fraud (FN)

## Model Building  

Firstly, lost revenue when no fraud is detected is calculated. Three versions of each algorithm with different variables and parameters will be evaluated. The best performing construct of each version will be chosen for the final evaluation.  

**No Fraud Predicted**: No fraud was predicted, all positive outcomes assumed. (All is_fraud = 0.)  

```{r No Fraud Detection System}
# Create vector predictions containing 0 for every transfer
predictions <- factor(rep(0, times = nrow(test2)), levels = c(0, 1))
```

```{r Confusion Matrix for No Fraud Detection System, digits = 3}  
# Compute accuracy
acc <- (confusionMatrix(predictions, test2$is_fraud)$overall[["Accuracy"]])*100

# Compute cost of not detecting fraudulent transactions from test2
cost <- sum(test2$amt[test2$is_fraud == 1])

# Compute cost of not detecting fraudulent transactions from test_set 
cost2 <- sum(test_set$amt[test_set$is_fraud == 1])
```  

The cost of not detecting fraud =  $ `r cost`.  
Accuracy with no correct fraud predictions: `r acc` %.

```{r Results for No Fraud Detection System}
# Tabulating fraud predictions
cost_preds <- tibble(amt = test2$amt, 
                     results = test2$is_fraud, 
                     no_preds = predictions)

# Tabulating cost results
cost_results <- tibble(Model = "No Fraud Detection",
                       AmtSaved = 0,
                       FraudMissed = cost,
                       Misclassified = 0,
                       SavedPct = 0,
                       MisclassPct = 0,
                       Specificity = 0,
                       NPV = 0)

# Removing large element
rm(predictions)
```

\newpage

### Rpart Models 
  
**Rpart Model 1**: To begin, all possible predictors will be included. The results and variable importance will also be accessed.

```{r Rpart Model 1}
# Fit the training data to the rpart model
# This takes all variables except trans_date_trans_time
fit_rpart_all <- train2 %>% 
  select(-c(trans_date_trans_time)) %>% 
  rpart(is_fraud ~ ., data = ., method = "class")

# Rpart predictions
yhat_rpart_all <- predict(fit_rpart_all, test2, type = "class")
```
  
Rpart Model 1 Confusion Matrix:
```{r Rpart Model 1 Confusion Matrix} 
# Tabulate Confusion Matrix table
kable(confusionMatrix(yhat_rpart_all, reference = test2$is_fraud)$table,
      escape = FALSE,
      align = "r") %>%
  kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

Rpart Model 1 Results:
```{r Rpart Model 1 Results}
# Calculating Model Costs
cost_preds2 <- cbind(cost_preds, yhat_rpart_all)
rp_all_saved <- cost_preds2 %>%
  filter(results == 1 & yhat_rpart_all == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rp_all_missed <- cost_preds2 %>%
  filter(results == 1 & yhat_rpart_all == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rp_all_misclassified <- cost_preds2 %>%
  filter(results == 0 & yhat_rpart_all == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rp_all_savedpct <- rp_all_saved/cost
rp_all_misclassifiedpct <- rp_all_misclassified/cost

# Evaluating model performance 
rp_all_specificity <- confusionMatrix(yhat_rpart_all, 
                                      reference = test2$is_fraud)$byClass[["Specificity"]]
rp_all_NPV <- confusionMatrix(yhat_rpart_all, 
                              reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

# Saving results in a table
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "Rpart Model 1",
                                 AmtSaved = rp_all_saved,
                                 FraudMissed = rp_all_missed,
                                 Misclassified = rp_all_misclassified,
                                 SavedPct = rp_all_savedpct,
                                 MisclassPct = rp_all_misclassifiedpct,
                                 Specificity = rp_all_specificity,
                                 NPV = rp_all_NPV))

kable(cost_results %>% 
        filter(Model == "Rpart Model 1"), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```


The results are not very impressive with only 67.9% of fraud amounts saved and just 60.2% actual fraudulent transactions detected. NPV result is 88.6% which shows that the model does a fairly decent job at not incorrectly flagging legitimate transactions as fraud.

One of the benefits of rpart is its ease of interpretability when plotting the decision tree.  Unfortunately, a model with a high level of classifiers such as this does not make a readable decision tree. Instead, evaluation of variable importance will be used instead.

Rpart Model 1 Variable Importance:
```{r Rpart Model 1 Variable Importance} 
# Var importance
kable(as.data.frame(fit_rpart_all$variable.importance), col.names = NULL) %>%
      kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
``` 
  
It is surprising that merchant is at the top the list and that category and bins are improving the model more than amount. This may be because the high-level categorical variables are not performing well. By including a constructed feature like bins, rpart's ability to calculate best splits is most likely inhibited.

**Rpart Model 2**:  For a simpler model, only amount, category and date parts are included as  predictors. Binning is not included in order to allow rpart partitioning to calculate best split value.  

```{r Rpart Model 2}
# Basic model with only amount, category and date parts variables
fit_rpart <- train2 %>%
  select(-c(trans_date_trans_time, cc_num, merchant, bins)) %>%
  rpart(is_fraud ~ ., data = ., method = "class")

# Rpart Model 2 predictions
yhat_rpart <- predict(fit_rpart, test2, type = "class")
```

Rpart Model 2 Confusion Matrix:
```{r Rpart Model 2 Confusion Matrix} 
# Tabulate Confusion Matrix table
kable(confusionMatrix(yhat_rpart, reference = test2$is_fraud)$table,
      escape = FALSE,
      align = "r") %>%
  kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

Rpart Model 2 Results:
```{r Rpart Model 2 Results}
# Calculating Model 2 Costs
cost_preds3 <- cbind(cost_preds2, yhat_rpart)
rp_saved <- cost_preds3 %>%
  filter(results == 1 & yhat_rpart == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rp_missed <- cost_preds3 %>%
  filter(results == 1 & yhat_rpart == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rp_misclassified <- cost_preds3 %>%
  filter(results == 0 & yhat_rpart == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rp_savedpct = rp_saved/cost
rp_misclassifiedpct = rp_misclassified/cost
  
# Evaluating model 2 performance 
rp_specificity <- confusionMatrix(yhat_rpart, 
                                  reference = test2$is_fraud)$byClass[["Specificity"]]
rp_NPV <- confusionMatrix(yhat_rpart, 
                          reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

# Saving results in a table
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "Rpart Model 2",
                                     AmtSaved = rp_saved,
                                     FraudMissed = rp_missed,
                                     Misclassified = rp_misclassified,
                                     SavedPct = rp_savedpct,
                                     MisclassPct = rp_misclassifiedpct,
                                     Specificity = rp_specificity,
                                     NPV = rp_NPV))

kable(cost_results %>% 
        filter(Model == "Rpart Model 2"), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

This is a significant improvement over the first model with 77.0% of fraudulent transaction amounts detected and specificity of 66.1%. With such a large dataset and high-level variables, should the complexity parameter (cp) be tuned to improve the model? 

When we plot the cp against the cross-validation error, it does appear that we could improve the model by decreasing the complexity parameter.$^{[7]}$

```{r Rpart Model 2 CP Plot, fig.width = 2.75, fig.height = 1.75}   
# Plotting cp vs cross validation error
as.data.frame(fit_rpart$cptable) %>%
  ggplot(aes(x = CP,y = xerror)) +
  geom_point(alpha = 0.5) + 
  geom_line(colour = "darkorange2") + 
  ylim(0.35,1) + 
  scale_x_reverse() + 
  labs(x = "Complexity Parameter", y = "Cross-validation Error") + 
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

**Rpart Model 2 Tuning CP**: Setting minimum split and complexity parameters to zero to determine which cp value minimizes the cross-validation error (xerror), then prune the model accordingly with prune.rpart function. 

```{r Rpart Model 2 CP Plot 1}
# Rpart Model 2 tuning cp with only amount, category and date parts variables
fit_rpartcp <- train2 %>% 
  select(-c(trans_date_trans_time, merchant, cc_num, bins)) %>% 
  rpart(is_fraud ~ ., data = ., minsplit = 0, cp = 0, method = "class")

# Plotting cp/err 
plotcp(fit_rpartcp)
```

```{r Rpart Model 2 CP Plot 2, fig.width = 2.75, fig.height = 1.75} 
# Plotting cp/err 
as.data.frame(fit_rpartcp$cptable) %>%
  ggplot(aes(x = CP,y = xerror)) +
  geom_point(alpha = 0.5) + 
  geom_line(colour = "darkorange2") +  
  scale_x_reverse() + 
  labs(x = "Complexity Parameter",y = "Cross-validation Error") + 
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

It appears that the xerror is minimized much below the complexity parameter default of 0.01 and there are multiple xerror values below 0.4. 

```{r Determine optimal Rpart Model 2 CP value}
cptable <- fit_rpartcp$cptable
n <- cptable[which.min(cptable[,"xerror"]),][["nsplit"]]

# Choosing cp with fewer splits 
rp_cp <- as.data.frame(fit_rpartcp$cptable) %>% 
  filter(nsplit == n) %>% 
  pull(CP)
```

```{r Prune Rpart Model 2 CP}
# Pruning basic rpart model   
pfit_rpartcp <- prune.rpart(fit_rpartcp, cp = rp_cp)

# Rpart Model 2 cp predictions
yhat_rpartcp <- predict(pfit_rpartcp, test2, type = "class")
```

Rpart Model 2 & Rpart Model 2 Tuned CP Confusion Matrices Compared:
```{r Rpart Model 2 vs Rpart Model 2 CP}  
# Tabulate Rpart Model 2 cp Confusion Matrix
cm_rpcp <- as.data.frame.matrix(confusionMatrix(yhat_rpartcp, test2$is_fraud)$table)

# Tabulate Rpart Model 2 Confusion Matrix
cm_rp <- as.data.frame.matrix(confusionMatrix(yhat_rpart, test2$is_fraud)$table)

# Combining the two confusion matrices
combined <- cbind(cm_rp, cm_rpcp)

# Print final comparison table
kable(combined) %>%
  add_header_above(c(" " = 1, "Rpart Model 2" = 2,
                     "Rpart Model 2: Tuned" = 2)) %>%
   kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

Rpart Model 2 Tuned CP Results Compared:
```{r Rpart Model 2 Tuned CP Results Compared}
# Calculating Model 2 Tuned CP Costs
cost_preds4 <- cbind(cost_preds3, yhat_rpartcp)
rpcp_saved <- cost_preds4 %>%
  filter(results == 1 & yhat_rpartcp == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rpcp_missed <- cost_preds4 %>%
  filter(results == 1 & yhat_rpartcp == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rpcp_misclassified <- cost_preds4 %>%
  filter(results == 0 & yhat_rpartcp == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rpcp_savedpct <- rpcp_saved/cost
rpcp_misclassifiedpct <- rpcp_misclassified/cost

# Evaluating model 2 cp performance 
rpcp_specificity <- confusionMatrix(yhat_rpartcp, 
                                    reference = test2$is_fraud)$byClass[["Specificity"]]
rpcp_NPV <- confusionMatrix(yhat_rpartcp, 
                            reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

# Saving results in a table
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "Rpart Model 2: Tuned",
                                     AmtSaved = rpcp_saved,
                                     FraudMissed = rpcp_missed,
                                     Misclassified = rpcp_misclassified,
                                     SavedPct = rpcp_savedpct,
                                     MisclassPct = rpcp_misclassifiedpct,
                                     Specificity = rpcp_specificity,
                                     NPV = rpcp_NPV))

kable(cost_results %>% 
        filter(Model == c("Rpart Model 2",
                          "Rpart Model 2: Tuned")), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position")) 
```

Tuning the complexity parameter slightly improved the saved amount by over \$10,000. In proportion wise, the tuned model helped saved 79.4% of all fraudulent transaction amounts. It had a much larger impact on reducing misclassified (false negative) amounts by over \$20,000. 

**Rpart Model 3**: Let's run the model with cc_num. In most models using a unique identifier is not advised. However, in this case, the cc_num has thousands of observations associated to it. Furthermore, because of the nature of fraud charges where fraudulent transactions are more likely to repeat on the same credit card, identifying fraud on a cc_num could be a valid predictor. 

```{r Rpart Model 3 CC}
# Verifying cc_num as predictor. no merchant, bins, trans_date\time
fit_rpart_cc <- train2 %>%
  select(-c(trans_date_trans_time, merchant, bins)) %>%
  rpart(is_fraud ~ ., data = ., method = "class")

# Rpart Model 3 predictions
yhat_rpart_cc <- predict(fit_rpart_cc, test2, type = "class")
```

Rpart Model 2 & 3 Confusion Matrices Compared:
```{r Rpart Model 3 CC Confusion Matrix}  
# Confusion Matrix for Rpart Model 3 CC
cm_rpcc <- as.data.frame.matrix(confusionMatrix(yhat_rpart_cc, test2$is_fraud)$table)

# Combining confusion matrices
combined2 <-cbind(cm_rp, cm_rpcc)

# Print final comparison table
kable(combined2) %>%
  add_header_above(c(" " = 1, "Rpart Model 2" = 2, 
                     "Rpart Model 3: CC" = 2)) %>%
  kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

Rpart Model 2 & 3 Results Compared:
```{r Rpart Model 2 & 3 CC Results}
# Calculating Rpart Model 3 costs
cost_preds5 <- cbind(cost_preds4, yhat_rpart_cc)
rpcc_saved <- cost_preds5 %>%
  filter(results == 1 & yhat_rpart_cc == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rpcc_missed <- cost_preds5 %>%
  filter(results == 1 & yhat_rpart_cc == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rpcc_misclassified <- cost_preds5 %>%
  filter(results == 0 & yhat_rpart_cc == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rpcc_savedpct <- rpcc_saved/cost
rpcc_misclassifiedpct <- rpcc_misclassified/cost

# Evaluating Rpart Model 3 performance
rpcc_specificity <- confusionMatrix(yhat_rpart_cc, 
                                    reference = test2$is_fraud)$byClass[["Specificity"]]
rpcc_NPV <- confusionMatrix(yhat_rpart_cc, 
                            reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

# Saving results in a table
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "Rpart Model 3: CC",
                                     AmtSaved = rpcc_saved,
                                     FraudMissed = rpcc_missed,
                                     Misclassified = rpcc_misclassified,
                                     SavedPct = rpcc_savedpct,
                                     MisclassPct = rpcc_misclassifiedpct,
                                     Specificity = rpcc_specificity,
                                     NPV = rpcc_NPV))

kable(cost_results %>% 
        filter(Model == c("Rpart Model 2",
                          "Rpart Model 3: CC")), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position")) 
```

The rpart model with credit card numbers performed close to but not as good as the Rpart Model 2 which included amount, category and data variables as predictors. By plotting the complexity parameter again, we can show that the default cp is not optimized.

```{r Rpart Model 3 CP Plot 1}
# Plotting cp/xerror
plotcp(fit_rpart_cc)
```

```{r Rpart Model 3 CP Plot 2, fig.width = 2.75, fig.height = 1.75}
as.data.frame(fit_rpart_cc$cptable) %>%
  ggplot(aes(x = CP, y = xerror)) +
  geom_point(alpha = 0.5) +
  geom_line(colour = "darkorange2") +
  scale_x_reverse() + 
  labs(x = "Complexity Parameter",y = "Cross-validation Error") + 
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

The rpart model with credit card numbers performed close to but not as good as the Rpart Model 2 with amount, category and data variables as predictors. Plotting the complexity parameter again, shows the error was still decreasing when the cp parameter was reached.

**Rpart Model 3 Tuning CP**: Setting minimum split and complexity parameters to zero to determine which cp value minimizes the cross-validation error (xerror), then prune the model accordingly with prune.rpart function. 

```{r Rpart Model 3 CC CP}
# Rpart Model 3 CC tuning cp to verify cc_num as predictor. no merchant, bins, trans_date\time
fit_rpartcp_cc <- train2 %>% select(-c(trans_date_trans_time, merchant, bins)) %>% 
  rpart(is_fraud ~ ., data = ., minsplit = 0, cp = 0, method = "class")

# Plotting cp/err
as.data.frame(fit_rpartcp_cc$cptable) %>%
  ggplot(aes(x = CP, y = xerror)) +
  geom_point(alpha = 0.5) +
  geom_line(colour = "darkorange2") +
  scale_x_reverse() + 
  labs(x = "Complexity Parameter",y = "Cross-validation Error") + 
  theme_bw(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank())
```

```{r Determine optimal Rpart Model 3 CC CP value}
cc_cptable <- fit_rpartcp_cc$cptable
cc_n <- cc_cptable[which.min(cc_cptable[,"xerror"]),][["nsplit"]]

# Choosing cp with fewer splits 
rpcc_cp <- as.data.frame(cc_cptable) %>% 
  filter(nsplit == cc_n) %>% 
  pull(CP)
```

```{r Prune Rpart Model 3 CC CP}
# Pruning Rpart Model 3 
pfit_rpartcc_cp <- prune.rpart(fit_rpartcp_cc, cp = rpcc_cp)

# Rpart Model 3 cp predictions
yhat_rpartcc_cp <- predict(pfit_rpartcc_cp, test2, type = "class")
```

Rpart Model 3 & Rpart Model 3 CP Tuned Confusion Matrices Compared:
```{r Rpart Model 3 CC vs Rpart Model 3 CC CP}
# Tabulate Rpart Model 3 CC CP Confusion Matrix
cm_rpcc_cp <- as.data.frame.matrix(confusionMatrix(yhat_rpartcc_cp, test2$is_fraud)$table)

# Combining Confusion Matrices
combined3 <- cbind(cm_rpcc, cm_rpcc_cp)

# Print final comparison table
kable(combined3) %>%
  add_header_above(c(" " = 1, "Rpart Model 3: CC" = 2,
                     "Rpart Model 3: Tuned" = 2)) %>%
   kable_styling(font_size = 9, bootstrap_options = "bordered", latex_options = c("striped", "HOLD_position"))
```

Rpart Model 3 CC & 3 Tuned CP Results Compared:
```{r Rpart Model 3 CC & CC Tuned Results}
# Calculating Rpart Model 3 CP Tuned costs
cost_preds6 <- cbind(cost_preds5, yhat_rpartcc_cp)
rpcc_cpsaved <- cost_preds6 %>%
  filter(results == 1 & yhat_rpartcc_cp == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rpcc_cpmissed <- cost_preds6 %>%
  filter(results == 1 & yhat_rpartcc_cp == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rpcc_cpmisclassified <- cost_preds6 %>%
  filter(results == 0 & yhat_rpartcc_cp == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rpcc_cpsavedpct <- rpcc_cpsaved/cost
rpcc_cpmisclassifiedpct <- rpcc_cpmisclassified/cost

# Evaluating Rpart Model 3 CP Tuned performance
rpcc_cp_specificity <- confusionMatrix(yhat_rpartcc_cp, 
                                       reference = test2$is_fraud)$byClass[["Specificity"]]
rpcc_cp_NPV <- confusionMatrix(yhat_rpartcc_cp, 
                            reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

# Saving results in a table
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "Rpart Model 3: Tuned",
                                     AmtSaved = rpcc_cpsaved,
                                     FraudMissed = rpcc_cpmissed,
                                     Misclassified = rpcc_cpmisclassified,
                                     SavedPct = rpcc_cpsavedpct,
                                     MisclassPct = rpcc_cpmisclassifiedpct,
                                     Specificity = rpcc_cp_specificity,
                                     NPV = rpcc_cp_NPV))

kable(cost_results %>% 
        filter(Model == c("Rpart Model 3: CC",
                          "Rpart Model 3: Tuned")), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position")) 
```

Unsurprisingly, the tuned model with card numbers performed the better than the untuned model with 76.8% of fraudulent transaction amounts saved. However, it still did not beat the tuned Rpart Model 2 with amount, category and date variables as predictors. Therefore, the Rpart Model 2 with amount, category and date variables as predictors will be used for the final validation and model comparison.  

```{r Final Validation Set up}
# Final Validation Set Up 
# Tabulating results & amounts
final_preds <- tibble(amt = test_set$amt, results = test_set$is_fraud)

# Tabulating final cost results
final_results <- tibble(Model = "No Fraud Predicted", 
                        AmtSaved = 0, 
                        FraudMissed = cost, 
                        Misclassified = 0, 
                        SavedPct = 0, 
                        MisclassPct = 0, 
                        Specificity = 0, 
                        NPV = 0)
```

```{r Final Validation Rpart}
# Rpart Model 2 cp predictions
yhat_rpartcp <- predict(pfit_rpartcp, test_set, type = "class")

# Tabulate Rpart Model 2 CP Confusion Matrix
cm_rp <- as.data.frame.matrix(confusionMatrix(yhat_rpartcp, 
                                              reference = test_set$is_fraud)$table)

# Calculating Rpart Model 2 CP Tuned costs
final_preds <- cbind(final_preds, yhat_rpartcp)
rpcp_saved <- final_preds %>%
  filter(results == 1 & yhat_rpartcp == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rpcp_missed <- final_preds %>%
  filter(results == 1 & yhat_rpartcp == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rpcp_misclassified <- final_preds %>%
  filter(results == 0 & yhat_rpartcp == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rpcp_savedpct <- rpcp_saved/cost2
rpcp_misclassifiedpct <- rpcp_misclassified/cost2

# Evaluating model performance
rpcp_specificity <- confusionMatrix(yhat_rpartcp, 
                                    reference = test_set$is_fraud)$byClass[["Specificity"]]
rpcp_NPV <- confusionMatrix(yhat_rpartcp, 
                            reference = test_set$is_fraud)$byClass[["Neg Pred Value"]]

# Saving results in final table
final_results <- bind_rows(final_results,
                           data.frame(Model = "Rpart Model 2: Tuned CP",
                                      AmtSaved = rpcp_saved,
                                      FraudMissed = rpcp_missed,
                                      Misclassified = rpcp_misclassified,
                                      SavedPct = rpcp_savedpct,
                                      MisclassPct = rpcp_misclassifiedpct,
                                      Specificity = rpcp_specificity,
                                      NPV = rpcp_NPV))

cost_preds <- cost_preds6
```

\newpage

### GLM Models  

To begin, two models will be compared to emphasize the limitations of glm models in machine learning. Since glm cannot handle high levels of categorical variables, merchant and cc numbers cannot be used. With glm models, we can first create the fit model then calculate probability estimates. Finally, a vector of predicted outcomes based on a threshold can then be created.  

**GLM Model 1**: The first model will include amount and category interaction along with date variables as factors. It does not include the additional calculated bins variable. Predicted outcomes based on the threshold > 0.5. 

```{r GLM Model 1}
# GLM model 1 fits amount & category interaction and date variables as factors
fit_glm1 <- train2 %>% 
  glm(is_fraud ~ category * amt + hour + day + month + weekday, data = ., 
      family = "binomial")

# GLM model 1 predictions
phat_glm1 <- predict.glm(fit_glm1, test2, type = "response")
yhat_glm1 <- ifelse(phat_glm1 > 0.5, 1, 0) %>% 
  factor()
```

**GLM Model 2**: The second model will be a multivariate linear model including the calculated feature bins but not modeling for any interaction.

```{r GLM Model 2}
# GLM model 2 fits category & amt interaction, no bins, date parts as factors
fit_glm2 <- train2 %>% 
  glm(is_fraud ~ category + amt + bins + hour + day + month + weekday, data = ., 
      family = "binomial")

# GLM model 2 predictions
phat_glm2 <- predict.glm(fit_glm2, test2, type = "response")
yhat_glm2 <- ifelse(phat_glm2 > 0.5, 1, 0) %>% 
  factor()
```

GLM Models 1 & 2 Confusion Matrices:
```{r GLM 1 & 2 Confusion Matrices}
# Confusion Matrix GLM Model 1 & 2
cm_glm1 <- confusionMatrix(yhat_glm1, reference = test2$is_fraud)$table
cm_glm2 <- confusionMatrix(yhat_glm2, reference = test2$is_fraud)$table

# Combining confusion matrices
combined4 <-cbind(cm_glm1, cm_glm2)

# Printing GLM model 1& 2 Confusion Matrices
kable(combined4) %>%
  add_header_above(c(" " = 1, "GLM Model 1: category * amount" = 2, 
                     "GLM Model 2: category + amount" = 2)) %>%
  kable_styling(font_size = 9, bootstrap_options = "bordered", latex_options = c("striped", "HOLD_position"))
```

GLM Models 1 & 2 Results:
```{r GLM 1 & 2 Results}
# Evaluating GLM models 1 & 2 performance
glm1_specificity <- confusionMatrix(yhat_glm1, 
                                    reference = test2$is_fraud)$byClass[["Specificity"]]
glm1_NPV <- confusionMatrix(yhat_glm1, 
                            reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

glm2_specificity <- confusionMatrix(yhat_glm2, 
                                    reference = test2$is_fraud)$byClass[["Specificity"]]
glm2_NPV <- confusionMatrix(yhat_glm2, 
                            reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

# Calculating GLM Model 1 Costs
cost_preds <- cbind(cost_preds, yhat_glm1, yhat_glm2)
glm1_saved <- cost_preds %>%
  filter(results == 1 & yhat_glm1 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm1_missed <- cost_preds %>%
  filter(results == 1 & yhat_glm1 == 0) %>%
  summarize(sum(amt)) %>%
  pull()
glm1_misclassified <- cost_preds %>%
  filter(results == 0 & yhat_glm1 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm1_savedpct <- glm1_saved/cost
glm1_misclassifiedpct <- glm1_misclassified/cost

# Calculating GLM Model 2 Costs
glm2_saved <- cost_preds %>%
  filter(results == 1 & yhat_glm2 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm2_missed <- cost_preds %>%
  filter(results == 1 & yhat_glm2 == 0) %>%
  summarize(sum(amt)) %>%
  pull()
glm2_misclassified <- cost_preds %>%
  filter(results == 0 & yhat_glm2 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm2_savedpct <- glm2_saved/cost
glm2_misclassifiedpct <- glm2_misclassified/cost

# Saving results to table
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "GLM Model 1: category * amount",
                                     AmtSaved = glm1_saved,
                                     FraudMissed = glm1_missed,
                                     Misclassified = glm1_misclassified,
                                     SavedPct = glm1_savedpct,
                                     MisclassPct = glm1_misclassifiedpct,
                                     Specificity = glm1_specificity,
                                     NPV = glm1_NPV))
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "GLM Model 2: category + amount",
                                     AmtSaved = glm2_saved,
                                     FraudMissed = glm2_missed,
                                     Misclassified = glm2_misclassified,
                                     SavedPct = glm2_savedpct,
                                     MisclassPct = glm2_misclassifiedpct,
                                     Specificity = glm2_specificity,
                                     NPV = glm2_NPV))

kable(cost_results %>% 
        filter(Model == c("GLM Model 1: category * amount",
                          "GLM Model 2: category + amount")), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

Even with interaction between category and amount, GLM Model 1 did not perform very well. GLM Model 2 without interaction performed better when a feature (i.e. bins) was introduced to capture the differences in fraud amounts in different categories. 

Linear models will only estimate relationships of the features provided. Rpart performed better without the added bin construct because the algorithm calculates its own splits. For glm we must know our data well and provide appropriate predictors that best fit the algorithm and data structure.

**GLM Model 3**: This model will include amount/category/bin interactions plus date variables as factors. The model's estimates will then be evaluated at thresholds of 0.5 and 0.4 for comparison.

```{r Load GLM Model 3 & Random Forest Model 2}
# Load the generated random forest model 2 if processing computer does not have enough RAM

##### Loading Generated Models #####

# If you want to run the code, but not spend the time processing the models:
# This will download fit models and load them into your RStudio environment.

# create temp file & url 
dl <- tempfile()
URL <- "https://github.com/Wilsven/Credit-Card-Fraud-Detection/releases/download/v1-files/CYO_Models.RData"

# download url into temp file
download.file(URL, dl)

# load .RData into environment. This may take a few minutes.
load(dl)

# Model fitted with category, amount and bins interactions, including all date variables
# fit_glm3 <- train2 %>%
# glm(is_fraud ~ category * amt * bins + hour + day + month + weekday, data = ., 
#    family = "binomial", model = FALSE, y = FALSE)
fit_glm3 <- fit_glm_bins

# GLM Model 3 estimates
test2_temp <- test2 %>%
  rename(trans_hour = hour)
phat_glm3 <- predict.glm(fit_glm3, test2_temp, type = "response")

# GLM Model 3 predictions at 0.5
yhat_glm3_.5 <- ifelse(phat_glm3 > 0.5, 1, 0) %>% 
  factor()
# GLM Model 3 predictions at 0.4
yhat_glm3_.4 <- ifelse(phat_glm3 > 0.4, 1, 0) %>% 
  factor()

# Removing large elements
rm(dl, URL, fit_glm1, fit_glm2, fit_rf51)
```
 
GLM Model 3 Confusion Matrices:
```{r GLM 3 Confusion Matrices}
# GLM Model 3 Confusion Matrices
cm_glm3_.5 <- as.data.frame.matrix(confusionMatrix(yhat_glm3_.5, 
                                                   reference = test2$is_fraud)$table)
cm_glm3_.4 <- as.data.frame.matrix(confusionMatrix(yhat_glm3_.4, 
                                                   reference = test2$is_fraud)$table)

combined5 <- cbind(cm_glm3_.5, cm_glm3_.4)

# Printing GLM Model 1 & 2 Confusion Matrices
kable(combined5) %>%
  add_header_above(c(" " = 1,"GLM Model 3: > 0.5" = 2,"GLM Model 3: > 0.4" = 2)) %>%
  kable_styling(font_size = 9, bootstrap_options = "bordered", latex_options = c("striped", "HOLD_position"))

rm(cm_glm3_.5, cm_glm3_.4)
```

GLM Model 3 Results:
```{r GLM Model 3 results}
# Evaluating GLM Model 3 performance
glm3_.5_specificity <- confusionMatrix(yhat_glm3_.5, 
                                       reference = test2$is_fraud)$byClass[["Specificity"]]
glm3_.4_specificity <- confusionMatrix(yhat_glm3_.4, 
                                       reference = test2$is_fraud)$byClass[["Specificity"]]
glm3_.5_NPV <- confusionMatrix(yhat_glm3_.5, 
                               reference = test2$is_fraud)$byClass[["Neg Pred Value"]]
glm3_.4_NPV <- confusionMatrix(yhat_glm3_.4, 
                               reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

cost_preds <- cbind(cost_preds, yhat_glm3_.5, yhat_glm3_.4)

# Calculating GLM Model 3 > 0.5 Costs
glm3_.5_saved <- cost_preds %>%
  filter(results == 1 & yhat_glm3_.5 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.5_missed <- cost_preds %>%
  filter(results == 1 & yhat_glm3_.5 == 0) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.5_misclassified <- cost_preds %>%
  filter(results == 0 & yhat_glm3_.5 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.5_savedpct <- glm3_.5_saved/cost
glm3_.5_misclassifiedpct <- glm3_.5_misclassified/cost

# Calculating GLM Model 3 > 0.4 Costs
glm3_.4_saved <- cost_preds %>%
  filter(results == 1 & yhat_glm3_.4 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.4_missed <- cost_preds %>%
  filter(results == 1 & yhat_glm3_.4 == 0) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.4_misclassified <- cost_preds %>%
  filter(results == 0 & yhat_glm3_.4 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.4_savedpct <- glm3_.4_saved/cost
glm3_.4_misclassifiedpct <- glm3_.4_misclassified/cost

# Saving results to table
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "GLM Model 3: > 0.5",
                                     AmtSaved = glm3_.5_saved,
                                     FraudMissed = glm3_.5_missed,
                                     Misclassified = glm3_.5_misclassified,
                                     SavedPct = glm3_.5_savedpct,
                                     MisclassPct = glm3_.5_misclassifiedpct,
                                     Specificity = glm3_.5_specificity,
                                     NPV = glm3_.5_NPV))
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "GLM Model 3: > 0.4",
                                     AmtSaved = glm3_.4_saved,
                                     FraudMissed = glm3_.4_missed,
                                     Misclassified = glm3_.4_misclassified,
                                     SavedPct = glm3_.4_savedpct,
                                     MisclassPct = glm3_.4_misclassifiedpct,
                                     Specificity = glm3_.4_specificity,
                                     NPV = glm3_.4_NPV))
# Printing Models results
kable(cost_results %>% 
        filter(Model == c("GLM Model 3: > 0.5", 
                          "GLM Model 3: > 0.4")), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

Reducing the probability threshold increased the number of correct fraud predictions and saved about \$18,000 more, but it increased false positives (i.e. misclassified) by more than \$20,000. This is where companies must decide on a trade-off. Is it better to catch more fraud at the risk of denying some legitimate transactions and possibly upsetting customers and losing customers. With today's automation, banks can send a text allowing the cardholder to approve or deny the suspicious transaction. Therefore, this report strongly believes that it is more important to reduce false negative (i.e. fraud undetected/missed) than false positives.

```{r GLM Model 3 Final Validation}
# Predictions
test_set_temp <- test_set %>%
  rename(trans_hour = hour)
phat_glm3 <- predict.glm(fit_glm3, test_set_temp, type = "response")
yhat_glm3_.4 <- ifelse(phat_glm3 > 0.4, 1, 0) %>%
  factor()

# Evaluating GLM Model 3 performance
cm_glm <- as.data.frame.matrix(confusionMatrix(yhat_glm3_.4, 
                                               reference = test_set$is_fraud)$table)
glm3_.4_specificity <- confusionMatrix(yhat_glm3_.4, 
                                        reference = test_set$is_fraud)$byClass[["Specificity"]]
glm3_.4_NPV <- confusionMatrix(yhat_glm3_.4, 
                                reference = test_set$is_fraud)$byClass[["Neg Pred Value"]]

# Calculating GLM Model 3 > 0.4 Costs
final_preds <- cbind(final_preds, yhat_glm3_.4)
glm3_.4_saved <- final_preds %>%
  filter(results == 1 & yhat_glm3_.4 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.4_missed <- final_preds %>%
  filter(results == 1 & yhat_glm3_.4 == 0) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.4_misclassified <- final_preds %>%
  filter(results == 0 & yhat_glm3_.4 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
glm3_.4_savedpct <- glm3_.4_saved/cost2
glm3_.4_misclassifiedpct <- glm3_.4_misclassified/cost2

# Saving results
final_results <- bind_rows(final_results,
                           data.frame(Model = "GLM Model 3: > 0.4",
                                      AmtSaved = glm3_.4_saved,
                                      FraudMissed = glm3_.4_missed,
                                      Misclassified = glm3_.4_misclassified,
                                      SavedPct = glm3_.4_savedpct,
                                      MisclassPct = glm3_.4_misclassifiedpct,
                                      Specificity = glm3_.4_specificity,
                                      NPV = glm3_.4_NPV))
```

\newpage

### RandomForest Models  
  
**RandomForest Model 1**: The main tuning parameter for random forest are the number of trees (ntree) and the number of variables to sample per tree (mtry). RandomForest defaults to 500 trees and the square root of the number of columns in the formula. Sampling can be done with or without replacement although more randomness will be generated with replacement. Due to machine memory limits, A very low number of trees (51) is opted.

```{r Random Forest Model 1}
# Resetting seed
set.seed(1, sample.kind = "Rounding")

# Random Forest Model 1 
fit_rf <- train2 %>% 
  randomForest(is_fraud ~ ., data = .,
               ntree = 51, 
               replacement = TRUE,
               importance = TRUE)

# Random FOrest Model 1 predictions
yhat_rf <- predict(fit_rf, test2, type = "class")
```

RandomForest Model 1 Confusion Matrix:
```{r RandomForest Model 1 Confusion Matrix}
kable(confusionMatrix(yhat_rf, 
                      reference = test2$is_fraud)$table, 
      escape = FALSE, 
      align = "r") %>%
      kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

RandomForest Model 1 Results: 
```{r RandomForest Model 1 Results}  
# Evaluating Random Forest Model 1 performance
rf_specificity <- confusionMatrix(yhat_rf, 
                                 reference = test2$is_fraud)$byClass[["Specificity"]]
rf_NPV <- confusionMatrix(yhat_rf, 
                          reference = test2$is_fraud)$byClass[["Neg Pred Value"]]

# Calculating Random Forest Model 1 Costs
cost_preds <- cbind(cost_preds, yhat_rf)
rf_saved <- cost_preds %>%
  filter(results == 1 & yhat_rf == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rf_missed <- cost_preds %>%
  filter(results == 1 & yhat_rf == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rf_misclassified <- cost_preds %>%
  filter(results == 0 & yhat_rf == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rf_savedpct <- rf_saved/cost
rf_misclassifiedpct <- rf_misclassified/cost

# Saving results
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "Random Forest Model 1",
                                     AmtSaved = rf_saved,
                                     FraudMissed = rf_missed,
                                     Misclassified = rf_misclassified,
                                     SavedPct = rf_savedpct,
                                     MisclassPct = rf_misclassifiedpct,
                                     Specificity = rf_specificity,
                                     NPV = rf_NPV))

kable(cost_results %>% 
        filter(Model == "Random Forest Model 1"), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

The first randomForest model performed extremely well even with only 51 trees. Plotting the error rate by class vs number of trees to see if there is room for improvement.$^{[8]}$ 
  
```{r Random Forest Model 1 Error Rate}
# Get OOB data from plot and coerce to data.table
oobData <- as.data.table(plot(fit_rf))

# Add new trees column into OOB data table
oobData[, trees := .I]

# min OOB & fraud class Error at mtry 3
mtry3_OOBErr <- min(oobData$OOB)*100
mtry3_FraudErr <- min(oobData$'1')*100

# Cast to long format
oobData <- melt(oobData, id.vars = "trees")
setnames(oobData, "value", "error")
```

```{r Random Forest Model 1 Plotting Error, fig.width = 5, fig.height = 1.75}  
# Plotting trees / error
oobData %>% 
  ggplot(aes(x = trees, y = error)) + 
  geom_line(colour = "darkorange2") + 
  facet_wrap(~ variable, scales = "free") +
  labs(title = "Out-of-Bag & Class Errors by Number of Trees") +
  theme_bw(base_size = 10) + 
  theme(plot.title = element_text(size = 9))
```

At mtry = 3 the minimum OOBError = `r mtry3_OOBErr` % and Fraud Class Error = `r mtry3_FraudErr` %.  It would be interesting to look at error rates for higher values of mtry to see if the results can be improved. In an imbalanced dataset, as with accuracy, OOB error is not particularly helpful as it is skewed to the majority class. In the graph above, you can see that legitimate transactions are gradually plateauing after 30 trees, but the plot for fraudulent transactions is still declining and has a much higher error rate.  

**RandomForest Model 1 Tuning mtry**: The tuneRF function will be used to tune the model.  The tuneRF function takes in a starting mtry input and returns the OOB error for a step factor above and below. The number of trees will also be increased slightly in this model.  

```{r RandomForest Tuning mtry}
## Tuning RF Model for best mtry value ##

# Tuning rf for mtry 4:6, 75 trees
rf_tune <-  tuneRF(train2[-6], train2$is_fraud, 
                   mtryStart = 5, 
                   ntreeTry = 75, 
                   stepFactor = 0.9,
                   trace = TRUE, 
                   plot = TRUE)
```

```{r Plotting Tune RandomForest, fig.width = 3, fig.height = 1.75}
options(digits = 6)

as.data.frame(rf_tune) %>% 
  mutate(OOBError = OOBError*100) %>%
  ggplot(aes(x = mtry, y = OOBError)) +
  geom_point() + 
  geom_line(colour = "#56B4E9") + 
  ylim(0.2, 0.21) +
  geom_text(aes(label=sprintf("%1.3f%%",OOBError)), size = 2.5,  nudge_y = .001,alpha =3/4) +
  scale_x_continuous(breaks=c(4,5,6))+ theme_bw(base_size = 10) + 
  theme(plot.title = element_text(size = 9)) + 
  ggtitle("OOB Error Rate by Mtry Value")
```

In actuality, we are more interested in the err.rate for fraud class and the cost results than the OOB Error. However, this does show that different values of mtry do perform better than the default. Using more trees would produce better results, but tuneRF is a very time consuming function. Since the OOBError value changes only at a hundredth of a percent, these models should produce very similar values. Using a smaller number of predictors is supposed to allow randomForest to pick up trends between predictors that would be less noticeable with the full set. Unfortunately, due to technical restrictions and computational limitations, the number of trees that can ran is restricted and this will inhibit the performance and also reproducibility of the results. Therefore, the next randomForest model will be trained with only 251 trees (just over half of the default) and use mtry = 4.  

```{r Load Random Forest Model 2}
# Load the generated random forest model 2 if processing computer does not have enough RAM

# create temp file & url 
dl <- tempfile()
URL <- "https://github.com/Wilsven/Credit-Card-Fraud-Detection/releases/download/v1-files/CYO_Models.RData"

# download url into temp file
download.file(URL, dl)

# load .RData into environment. This may take a few minutes.
load(dl)

# 251 trees, 4/10 predictors
# fit_rf251 <- train2 %>% 
#  randomForest(is_fraud ~., data = .,
#               ntree = 251, mtry = 4,
#               replacement = TRUE,
#               importance = TRUE)
fit_rf2 <- fit_rf251

# Random Forest Model 2 predictions 
test2_temp <- test2 %>%
  rename(trans_hour = hour)
yhat_rf2 <- predict(fit_rf2, test2_temp, type = "class")

# Remove files
rm(dl, URL, fit_glm1, fit_glm2, fit_glm3, fit_glm_bins, fit_rf51)
```

RandomForest Model 2 Confusion Matrix:
```{r RandomForest Model 2 Confusion Matrix}  
# Confusion Matrix
kable(confusionMatrix(yhat_rf2, 
                      reference = test2_temp$is_fraud)$table, 
      escape = FALSE, 
      align = "r") %>%
      kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

RandomForest Model 1 & 2 Results:
```{r RandomForest Model 1 & 2 Results}  
# Evaluating model performance
rf2_specificity <- confusionMatrix(yhat_rf2, 
                                   reference = test2_temp$is_fraud)$byClass[["Specificity"]]
rf2_NPV <- confusionMatrix(yhat_rf2, 
                           reference = test2_temp$is_fraud)$byClass[["Neg Pred Value"]]

# Calculating Random Forest Model 2 Costs
cost_preds <- cbind(cost_preds, yhat_rf2)
rf2_saved <- cost_preds %>%
  filter(results == 1 & yhat_rf2 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rf2_missed <- cost_preds %>%
  filter(results == 1 & yhat_rf2 == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rf2_misclassified <- cost_preds %>%
  filter(results == 0 & yhat_rf2 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rf2_savedpct <- rf2_saved/cost
rf2_misclassifiedpct <- rf2_misclassified/cost

# Saving results
cost_results <- bind_rows(cost_results,
                          data.frame(Model = "Random Forest Model 2",
                                     AmtSaved = rf2_saved,
                                     FraudMissed = rf2_missed,
                                     Misclassified = rf2_misclassified,
                                     SavedPct = rf2_savedpct,
                                     MisclassPct = rf2_misclassifiedpct,
                                     Specificity = rf2_specificity,
                                     NPV = rf2_NPV))

kable(cost_results %>% 
        filter(Model == c("Random Forest Model 1", "Random Forest Model 2")), 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))

# Removing large element
```

```{r Random Forest Model 2 Error Rate}  
# Get OOB data from plot and coerce to data.table
oobData <- as.data.table(plot(fit_rf2))

# Define trees as 1:ntree
oobData[, trees := .I]

# Cast to long format
oobData <- melt(oobData, id.vars = "trees")
setnames(oobData, "value", "error")
```    

Unsurprisingly, the randomforest models performed the best. It was also interesting to see that by increasing the variables per tree, false fraud predictions was reduced. It also slightly reduced the number of correct predictions and missed fraud. Looking at the error rate for the model: 

```{r Random Forest Model 2 Plotting Error, fig.width = 3.5, fig.height = 2}
# Plotting trees / error
oobData %>% 
  filter(variable == 1) %>%
  ggplot(aes(x = trees, y = error)) + 
  geom_line(colour = "darkorange2") + 
  ylim(0.26, 0.325) +
  labs(title = "Fraud Error Rate by Number of Trees") +
  theme_bw(base_size = 10) + 
  theme(plot.title = element_text(size = 9))
```

Adding more trees could still improve the model, but the error rate appears to be stabilizing. Therefore, the RandomForest Model 2 with 251 trees will be used for the final validation.  

**_Note on Variation of RandomForest Results_**: Even with setting a seed, there is still randomness introduced in the algorithm and the results may change slightly wand differ from run to run (setting the seed did create reproducible models results when ran on the same day). During testing, multiple randomForest 251 models with mtry of 4 and 5 were ran. The code and results were not included to save time, especially when this is a already lengthy project.  The mtry = 5 models had a lot of variation in their results and class 1 error. They sometimes performed thousands of dollars better or worse. The mtry = 4 models had less variation in both results and class error. A possible assumption could be that the mtry = 5 sometimes picked up very good trends and which caused over-fitting of the trees. This meant that while 251 trees can produce very good results, the number of trees is too low to fit a stable model. Unfortunately, due to computational limitations, rf models with large number of trees were not feasible. However, there are rf models online with 1000 and 1500 trees which produced much stable and better results!  

```{r RandomForest Model 2 Final Validation}
# Random Forest Model 2 predictions 
test_set_temp <- test_set %>%
  rename(trans_hour = hour)
yhat_rf2 <- predict(fit_rf2, test_set_temp, type = "class")

# Evaluating model performance
cm_rf <- as.data.frame.matrix(confusionMatrix(yhat_rf2, 
                                              reference = test_set_temp$is_fraud)$table)
rf2_specificity <- confusionMatrix(yhat_rf2, 
                                   reference = test_set_temp$is_fraud)$byClass[["Specificity"]]
rf2_NPV <- confusionMatrix(yhat_rf2, 
                           reference = test_set_temp$is_fraud)$byClass[["Neg Pred Value"]]

# Calculating Random Forest Model 2 Costs
final_preds <- cbind(final_preds, yhat_rf2)
rf2_saved <- final_preds %>%
  filter(results == 1 & yhat_rf2 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rf2_missed <- final_preds %>%
  filter(results == 1 & yhat_rf2 == 0) %>%
  summarize(sum(amt)) %>%
  pull()
rf2_misclassified <- final_preds %>%
  filter(results == 0 & yhat_rf2 == 1) %>%
  summarize(sum(amt)) %>%
  pull()
rf2_savedpct <- rf2_saved/cost2
rf2_misclassifiedpct <- rf2_misclassified/cost2

# Saving results
final_results <- bind_rows(final_results,
                          data.frame(Model = "Random Forest Model 2",
                                     AmtSaved = rf2_saved,
                                     FraudMissed = rf2_missed,
                                     Misclassified = rf2_misclassified,
                                     SavedPct = rf2_savedpct,
                                     MisclassPct = rf2_misclassifiedpct,
                                     Specificity = rf2_specificity,
                                     NPV = rf2_NPV))
```

\newpage

## Final Validation     

Each of the validation set containing 20% of original data which is not used in training the models or data exploration, will be fed to the chosen models for final validation.

  * Rpart with tuned complexity parameter
  * GLM with interaction between category, amount, and bins with predicted outcomes > 0.40
  * RandomForest with 251 trees sampling 4 predictors
  
**Final Validation Results**: Even with only half the default number of trees, random forest was able to perform the best and correctly predict 94.0% of the fraudulent transactions amounts. GLM was very close behind saving just \$7000 less by choosing predictions > .4, but it had the most misclassified amount overall. Even though rpart saved the least amount, it performed best at not misclassifying legitimate transactions.  

Final Models Confusion Matrices Comparison:
```{r Confusion Matrices}
# Combining confusion matrices
cms <- cbind(cm_rp, cm_glm, cm_rf)

# Printing final cms
kable(cms) %>%
  add_header_above(c(" " = 1, "Rpart" = 2,"GLM" = 2, "RandomForest" = 2)) %>%
  kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```  

Final Models Cost Results Comparison:
```{r Final Results}
# Printing final validation results
kable(final_results, 
      digits = 2, 
      escape = FALSE, 
      align = "r") %>%
    kable_styling(font_size = 9, latex_options = c("striped", "HOLD_position"))
```

## Conclusion  

With fraud detection, companies must decide on a balance between true positives (specificity), false positives (misclassified), false negatives and the resulting costs. As seen in the confusion matrices, adjusting models to increase the number of true fraud predictions often impacts missed fraud predictions and false fraud predictions. More importantly revenue saved or lost can vary more dramatically than the confusion matrix results show.  These models could continue to be tuned and improved, Unfortunately, and with only 251 trees randomForest does not return consistent results. Even this limited model, would have saved my synthetic credit card company about 83% which I think is a pretty successful beginning to credit card fraud detection model.  
  
Although this was a synthetic dataset without real-life predictors and cannot be used as an actual fraud detection model, many insights are able to be gained.  The size of the dataset did cause complications. Larger processing capability and memory would improve modeling. Also, additional cost-sensitive algorithms could be explored or synthetic over-sampling techniques such as SMOTE which may improve results.  Instead of rpart the party package could be used which models conditional probabilities more effectively.

In the end, even with this highly-imbalanced dataset and limited predictors, several models were created that had significant cost saving capabilities. Overall, I found this to be a very educational project into anomaly detection algorithms and effective metrics.  
  
\newpage  
  
## References  

[1] Irizarry, Rafael. (2021). [Machine Learning, Section 31.10: Classification and regression trees (CART). Introduction to Data Science.](https://rafalab.github.io/dsbook/examples-of-algorithms.html#classification-and-regression-trees-cart)

[2] Therneau, T.M., Atkinson, E.J. (April 11, 2019). [An Introduction to Recursive Partitioning Using the RPART Routines](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)

[3] Irizarry, Rafael. (2021). [Machine Learning, Section 31.11: Random forests. Introduction to Data Science.](https://rafalab.github.io/dsbook/examples-of-algorithms.html#random-forests)

[4] Liaw, Andy. (March 25, 2018). [Package 'randomForest'.](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf)

[5] Irizarry, Rafael. (2021). [Machine Learning, Section 31.3: Logistic Regression. Introduction to Data Science.](https://rafalab.github.io/dsbook/examples-of-algorithms.html#logistic-regression)

[6] Brownlee, Jason. (February 7, 2020). [Cost-Sensitive Learning for Imbalanced Classification.](https://machinelearningmastery.com/cost-sensitive-learning-for-imbalanced-classification/)

[7] Kabacoff, R.I. (2017). [Tree Bases Models.](https://www.statmethods.net/advstats/cart.html)

[8] Brownlee, Jason. (February 5, 2016). [Tune Machine Learning Algorithms in R (random forest case study).](https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/)